Weight: 8
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

Find the pod that consumes the most CPU and store the result to the file /opt/high_cpu_pod in the following format cluster_name,namespace,pod_name.

The pod could be in any namespace in any of the clusters that are currently configured on the student-node.

NOTE: It's recommended to wait for a few minutes to allow deployed objects to become fully operational and start consuming resources.

data stored in /opt/high_cpu_pod?


#!/bin/bash
  
# Define the output file path
output_file="/opt/high_cpu_pod"

# Initialize variables to store information about the pod with highest CPU usage
max_cpu_usage=0
high_cpu_pod=""

# Iterate over each configured cluster
for cluster in $(kubectl config get-clusters | grep -v NAME | sort); do
    echo "Checking cluster: $cluster"

    # Iterate over each namespace in the cluster
    for namespace in $(kubectl --context="$cluster" get namespaces -o=jsonpath='{.items[*].metadata.name}'); do
        echo "Checking namespace: $namespace"

        # Get CPU usage for each pod in the namespace
        pods_cpu_usage=$(kubectl --context="$cluster" -n "$namespace" top pods --no-headers | awk '{print $2,$1}')

        # Loop through each pod and store CPU usage
        while IFS= read -r pod_cpu_usage; do
            
            cpu_usage_raw=$(echo "$pod_cpu_usage" | awk '{print $1}')
            cpu_usage=$(echo "${cpu_usage_raw//m/}")  # Remove "mi" from the CPU usage
            cpu_usage_int=$(echo "$cpu_usage/1" | bc)  # Convert CPU usage to an integer
            
            pod_name=$(echo "$pod_cpu_usage" | awk '{print $2}')
            echo "Pod: $pod_name CPU usage: $cpu_usage"

            # Store the pod with the highest CPU consumption
            if (( $(echo "$cpu_usage > $max_cpu_usage" | bc -l) )); then
                max_cpu_usage=$cpu_usage
                high_cpu_pod="$cluster,$namespace,$pod_name"
            fi
        done <<< "$pods_cpu_usage"
    done
done

# Write the result to the output file
echo "$high_cpu_pod" > "$output_file"
echo "High CPU pod: $high_cpu_pod"

=================================================================================

Weight: 2
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster3 by running:

kubectl config use-context cluster3

Run a pod called alpine-sleeper-cka15-arch using the alpine image in the default namespace that will sleep for 7200 seconds.

k run alpine-sleeper-cka15-arch --image=alpine --namespace=default --command -- /bin/sh -c "sleep 7200"

=================================================================================

SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

Create a service account called deploy-cka20-arch. Further create a cluster role called deploy-role-cka20-arch with permissions to get the deployments in cluster1.

Finally create a cluster role binding called deploy-role-binding-cka20-arch to bind deploy-role-cka20-arch cluster role with deploy-cka20-arch service account.


deploy-cka20-arch.yaml: 

apiVersion: v1
kind: ServiceAccount
metadata:
  name: deploy-cka20-arch

deploy-role-cka20-arch.yaml:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: deploy-role-cka20-arch
rules:
- apiGroups: [""]
  resources: ["deployments"]
  verbs: ["get"]

deploy-role-binding-cka20-arch.yaml:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: eploy-role-binding-cka20-arch
subjects:
- kind: ServiceAccount
  name: deploy-cka20-arch
  namespace: default
roleRef:
  kind: ClusterRole
  name: deploy-role-cka20-arch
  apiGroup: rbac.authorization.k8s.io

k apply -f deploy-cka20-arch.yaml
k apply -f deploy-role-cka20-arch.yaml
k apply -f deploy-role-binding-cka20-arch.yaml

=================================================================================

Weight: 2
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster3 by running:

kubectl config use-context cluster3

Run a pod called looper-cka16-arch using the busybox image that runs the while loop while true; do echo hello; sleep 10;done. This pod should be created in the default namespace.

olution:

kubectl run looper-cka16-arch --image=busybox --namespace=default --command -- /bin/sh -c "while true; do echo hello; sleep 10;done"

check:

kubectl logs looper-cka16-arch 
hello
hello
hello
hello
....

or 

watch kubectl logs looper-cka16-arch

=================================================================================

Weight: 5
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

There is a sample script located at /root/service-cka25-arch.sh on the student-node.
Update this script to add a command to filter/display the targetPort only for service service-cka25-arch using jsonpath. The service has been created under the default namespace on cluster1

solution:

++++++++++++humun + chatgpt+++++++++++++++++

vi /root/service-cka25-arch.sh:

#!/bin/bash
namespace="default"
# Set the name of the service
service_name="service-cka25-arch"
# Retrieve the targetPort for the specified service using JSONPath
target_port=$(kubectl get svc "$service_name" -n "$namespace" -o=jsonpath='{.spec.ports[0].targetPort}')

echo $target_port


chmod +x /root/service-cka25-arch.sh
student-node ~ ➜  bash /root/service-cka25-arch.sh
9376

=================================================================================

Weight: 8
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

One of the nginx based pod called cyan-pod-cka28-trb is running under cyan-ns-cka28-trb namespace and it is exposed within the cluster using cyan-svc-cka28-trb service.

This is a restricted pod so a network policy called cyan-np-cka28-trb has been created in the same namespace to apply some restrictions on this pod.

Two other pods called cyan-white-cka28-trb and cyan-black-cka28-trb are also running in the default namespace.

The nginx based app running on the cyan-pod-cka28-trb pod is exposed internally on the default nginx port (80).

Expectation: This app should only be accessible from the cyan-white-cka28-trb pod.

Problem: This app is not accessible from anywhere.

Troubleshoot this issue and fix the connectivity as per the requirement listed above.

Note: You can exec into cyan-white-cka28-trb and cyan-black-cka28-trb pods and test connectivity using the curl utility.

You may update the network policy, but make sure it is not deleted from the cyan-ns-cka28-trb namespace.

App accessible from cyan-white-cka28-trb pod?
App NOT accessible from cyan-black-cka28-trb pod?
Network Policy still in use


debug autoremoving pod:

kubectl run -it --rm debug-pod --image=alpine -n cyan-ns-cka28-trb --overrides='{"apiVersion":"v1", "spec":{"nodeName":"cluster1-node01"}}' -- sh


updated NP but did not work:


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cyan-np-cka28-trb
  namespace: cyan-ns-cka28-trb
spec:
  podSelector:
    matchLabels:
      app: cyan-app-cka28-trb
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: cyan-white-cka28-trb
      ports:
        - protocol: TCP
          port: 80

fixed:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cyan-np-cka28-trb
  namespace: cyan-ns-cka28-trb
spec:
  podSelector:
    matchLabels:
      app: cyan-app-cka28-trb
  policyTypes:
    - Ingress
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: default
      - podSelector:
            matchLabels:
              app: cyan-white-cka28-trb
      ports:
        - protocol: TCP
          port: 80
        to:
        - ipBlock:
          cidr: 0.0.0.0/0  

=================================================================================

Weight: 1
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

There is a deployment called nginx-dp-cka04-trb which has been used to deploy a static website. The access to this website can be tested by running: curl http://kodekloud-exam.app:30002. However, it is not working at the moment.

Troubleshoot and fix it.

k describe pod nginx-dp-cka04-trb-767b767dc-rdjx5:

Warning  FailedMount  18s (x7 over 50s)  kubelet            MountVolume.SetUp failed for volume "nginx-config-volume-cka04-trb" : configmap "nginx-configuration-cka04-trb" not found

k get configmaps 
NAME                     DATA   AGE
index.html               1      2m27s
kube-root-ca.crt         1      96m
nginx-config-cka04-trb   1      2m27s

k get deployments.apps nginx-dp-cka04-trb -o yaml > nginx-dp-cka04-trb.yaml
edit nginx-dp-cka04-trb.yaml (find and fix from "nginx-configuration-cka04-trb" to "nginx-config-cka04-trb"):

k apply -f nginx-dp-cka04-trb.yaml

k delete pod -l app=nginx-cka04-trb
pod "nginx-dp-cka04-trb-767b767dc-rdjx5" deleted

student-node ~ ➜  curl http://kodekloud-exam.app:30002
</style>
</head>
<body>
<h1>Welcome to KodeKloud!</h1>
<p>If you see this page, the nginx web server is successfully 
installed and working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href=http://nginx.org/>nginx.org</a>.<br/>
Commercial support is available at
<a href=http://nginx.com/>nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>


??? how delete pod\s from deployment (after update) like a wildcard????
by his label, son:
k delete pod -l app=(app_name)

=================================================================================

Weight: 8
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

A template to create a Kubernetes pod is stored at /root/red-probe-cka12-trb.yaml on the student-node. However, using this template as-is is resulting in an error.

Fix the issue with this template and use it to create the pod. Once created, watch the pod for a minute or two to make sure its stable i.e, it's not crashing or restarting.

Make sure you do not update the args: section of the template.

Template is fixed and applied?
POD is stable?
Template is not manipulated?

vi /root/red-probe-cka12-trb.yaml:

apiVersion: v1
kind: Pod
metadata:
  name: red-probe-cka12-trb
spec:
  containers:
  - name: red-probe-cn-cka12-trb
    image: busybox:latest
    args:
    - /bin/sh
    - -c
    - sleep 3 ; touch /healthcheck; sleep 30;sleep 30000
    livenessProbe:
      httpGet:             			# fix to exec
          command:
          - cat
          - /healthcheck
      initialDelaySeconds: 1        # set value >= sleep 3 (i sat 5), if 1 pod will restart because 3 > 1 (see args)
      periodSeconds: 1
      failureThreshold: 1

k apply -f /root/red-probe-cka12-trb.yaml

pod/red-probe-cka12-trb                   1/1     Running   0          4m30s

=================================================================================

Weight: 5
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

There is an existing persistent volume called orange-pv-cka13-trb. A persistent volume claim called orange-pvc-cka13-trb is created to claim storage from orange-pv-cka13-trb.

However, this PVC is stuck in a Pending state. As of now, there is no data in the volume.

Troubleshoot and fix this issue, making sure that orange-pvc-cka13-trb PVC is in Bound state.


student-node ~ ➜  k describe persistentvolumeclaims 
Name:          orange-pvc-cka13-trb
Namespace:     default
StorageClass:  
Status:        Pending
Volume:        orange-pv-cka13-trb
Labels:        type=local
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      0
Access Modes:  
VolumeMode:    Filesystem
Used By:       <none>
Events:
  Type     Reason          Age                From                         Message
  ----     ------          ----               ----                         -------
  Warning  VolumeMismatch  10s (x4 over 51s)  persistentvolume-controller  Cannot bind to requested volume "orange-pv-cka13-trb": requested PV is too small

hahahahaha, classic...

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"labels":{"type":"local"},"name":"orange-pvc-cka13-trb","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"150Mi"}},"storageClassName":null,"volumeName":"orange-pv-cka13-trb"}}
  creationTimestamp: "2024-04-20T16:31:13Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    type: local
  name: orange-pvc-cka13-trb
  namespace: default
  resourceVersion: "12373"
  uid: b52efb90-15cc-47a2-b4bf-2b480923d3ab
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 150Mi        #set to <= 100Mi because size of orange-pv-cka13-trb == 100Mi
  volumeMode: Filesystem
  volumeName: orange-pv-cka13-trb
status:
  phase: Pending

k get persistentvolumeclaims 
NAME                   STATUS   VOLUME                CAPACITY   ACCESS MODES   STORAGECLASS   AGE
orange-pvc-cka13-trb   Bound    orange-pv-cka13-trb   100Mi      RWO                           17s

YEAH IAM LIZARD KING

=================================================================================

