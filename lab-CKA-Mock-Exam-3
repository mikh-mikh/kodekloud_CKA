Weight: 8
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

Create a service account called pink-sa-cka24-arch. Further create a cluster role called pink-role-cka24-arch with full permissions on all resources in the core api group under default namespace in cluster1.

Finally create a cluster role binding called pink-role-binding-cka24-arch to bind pink-role-cka24-arch cluster role with pink-sa-cka24-arch service account.

++++++++++++++chatgpt+++++++++++++++++++++++++
1. **Create ServiceAccount YAML** (pink-sa-cka24-arch.yaml):

apiVersion: v1
kind: ServiceAccount
metadata:
  name: pink-sa-cka24-arch

2. . **Create ClusterRole YAML** (pink-role-cka24-arch.yaml):

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pink-role-cka24-arch
rules:
- apiGroups: [""]
  resources: ["*"]
  verbs: ["*"]

3. **Create ClusterRoleBinding YAML** (pink-role-binding-cka24-arch.yaml):

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pink-role-binding-cka24-arch
subjects:
- kind: ServiceAccount
  name: pink-sa-cka24-arch
  namespace: default
roleRef:
  kind: ClusterRole
  name: pink-role-cka24-arch
  apiGroup: rbac.authorization.k8s.io

kubectl apply -f pink-sa-cka24-arch.yaml
kubectl apply -f pink-role-cka24-arch.yaml
kubectl apply -f pink-role-binding-cka24-arch.yaml

++++++++++++++++++++++++++++++++++++

========================================

Weight: 2
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster3 by running:

kubectl config use-context cluster3

Run a pod called alpine-sleeper-cka15-arch using the alpine image in the default namespace that will sleep for 7200 seconds.


kubectl run alpine-sleeper-cka15-arch --image=alpine --namespace=default --command -- /bin/sh -c "sleep 7200"


=========================================

Weight: 1
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

Create a generic secret called db-user-pass-cka17-arch in the default namespace on cluster1 using the contents of the file /opt/db-user-pass on the student-node

kubectl create secret generic db-user-pass-cka17-arch --from-file=/opt/db-user-pass

=========================================

Weight: 8
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

A pod called color-app-cka13-arch has been created in the default namespace. This pod logs can be accessed using kubectl logs -f color-app-cka13-arch command from the student-node. It is currently displaying Color is pink output. Update the pod definition file to make use of the environment variable with the value - green and recreate this pod.

Solution
Export the current pod definition:

student-node ~ ➜  kubectl get pod color-app-cka13-arch -o yaml > /tmp/color-app-cka13-arch.yaml



Edit the pod definition file to make the required changes:

student-node ~ ➜ vi /tmp/color-app-cka13-arch.yaml



Under env: -> - name: APP_COLOR change value: pink to value: green


Replace the pod:

student-node ~ ➜ kubectl replace -f /tmp/color-app-cka13-arch.yaml --force

pod "color-app-cka13-arch" deleted
pod/color-app-cka13-arch replaced

=========================================

Weight: 6
SECTION: ARCHITECTURE, INSTALL AND MAINTENANCE

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

A pod named beta-pod-cka01-arch has been created in the beta-cka01-arch namespace. Inspect the logs and save all logs starting with the string ERROR in file /root/beta-pod-cka01-arch_errors on the student-node.

k logs -n beta-cka01-arch beta-pod-cka01-arch | grep ERROR > /root/beta-pod-cka01-arch_errors

=========================================

Weight: 8
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

The deployment called web-dp-cka17-trb has 0 out of 1 pods up and running. Troubleshoot this issue and fix it. Make sure all required POD(s) are in running state and stable (not restarting).

The application runs on port 80 inside the container and is exposed on the node port 30090.

k describe pod web-dp-cka17-trb-6b5db94dc7-hcdf8
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  107s  default-scheduler  0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.


in this task needs to fix deployment (fix command, fix volumemounts), delete and recreate deployment, after need to fix PVC (pvc size > pv - it needs to set sizes: pvc <= pv)

k get deployments.apps web-dp-cka17-trb -o yaml > web-dp-cka17-trb.yaml
(edit and fix manifest errors)
k apply -f web-dp-cka17-trb.yaml
k get persistentvolumeclaims web-pvc-cka17-trb -o yaml > web-pvc-cka17-trb.yaml
(edit and fix manifest errors)
k apply -f web-pvc-cka17-trb.yaml
k get all:
NAME                                   READY   STATUS    RESTARTS   AGE
pod/color-app-cka13-arch               1/1     Running   0          30m
pod/web-dp-cka17-trb-b89ff4f58-h474n   1/1     Running   0          6s

NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP        67m
service/web-svc-cka17-trb   NodePort    10.102.123.255   <none>        80:30090/TCP   22m

=========================================

Weight: 2
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

A service account called deploy-cka19-trb is created in cluster1 along with a cluster role called deploy-cka19-trb-role. This role should have the permissions to get all the deployments under the default namespace. However, at the moment, it is not able to.

Find out what is wrong and correct it so that the deploy-cka19-trb service account is able to get deployments under default namespace.

++++++++++++++++++++my solution++++++++++++++++++++++++++++

1) k get clusterroles.rbac.authorization.k8s.io deploy-cka19-trb-role -o yaml > deploy-cka19-trb-role.yaml

2) vi deploy-cka19-trb-role.yaml:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: deploy-cka19-trb-role
rules:
- apiGroups:
  - ""
  resources:
  - deployments
  verbs:
  - "*"
3) k apply -f deploy-cka19-trb-role.yaml 
Warning: resource clusterroles/deploy-cka19-trb-role is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/deploy-cka19-trb-role configured # ok

4.5) vi deploy-cka19-trb-role-bind.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: blue-role-binding-cka19-rolebind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: deploy-cka19-trb-role
subjects:
- kind: ServiceAccount
  name: deploy-cka19-trb
  namespace: default

kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb

4) check:
k describe clusterroles.rbac.authorization.k8s.io deploy-cka19-trb-role 
Name:         deploy-cka19-trb-role
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources    Non-Resource URLs  Resource Names  Verbs
  ---------    -----------------  --------------  -----
  deployments  []                 []              [*]

kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb  


++++++++++++ KK Solution ++++++++++++++++++++++++
Let's see if deploy-cka19-trb service account is able to get the deployments.

kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb
We can see its not since we are getting no in the output.


Let's look into the cluster role:
kubectl get clusterrole deploy-cka19-trb-role -o yaml

The rules would be fine but we can see that there is no cluster role binding and service account associated with this. So let's create a cluster role binding.

kubectl create clusterrolebinding deploy-cka19-trb-role-binding --clusterrole=deploy-cka19-trb-role --serviceaccount=default:deploy-cka19-trb
Let's see if deploy-cka19-trb service account is able to get the deployments now.

kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy  

++++++++++++++++++++++++++++++++++++++++++++++++++


=========================================

Weight: 1
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

A YAML template for a Kubernetes deployment is stored at /root/app-cka07-trb.yaml. However, creating a deployment using this file is failing. Investigate the cause of the errors and fix the issue.
Make sure that the pod is in running state once deployed.  

Note: Do not to make any changes in the template file. #WTF

k apply -f /root/app-cka07-trb.yaml 
Error from server (NotFound): error when creating "/root/app-cka07-trb.yaml": namespaces "app-cka07-trb" not found
Error from server (NotFound): error when creating "/root/app-cka07-trb.yaml": namespaces "app-cka07-trb" not found

YEAH BOI:

1)vi app-cka07-trb-namespace.yaml:

apiVersion: v1
kind: Namespace
metadata:
  name: app-cka07-trb

2) k apply -f app-cka07-trb-namespace.yaml 
namespace/app-cka07-trb created

3) k apply -f app-cka07-trb.yaml 
configmap/index created
deployment.apps/app-deployment-cka07-trb created

k get all -n app-cka07-trb 
NAME                                           READY   STATUS    RESTARTS   AGE
pod/app-deployment-cka07-trb-d6546785b-h9gq6   1/1     Running   0          1m

=========================================

Weight: 11
SECTION: TROUBLESHOOTING

For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1

​A pod called nginx-cka01-trb is running in the default namespace. There is a container called nginx-container running inside this pod that uses the image nginx:latest. There is another sidecar container called logs-container that runs in this pod.

For some reason, this pod is continuously crashing. Identify the issue and fix it. Make sure that the pod is in a running state and you are able to access the website using the curl http://kodekloud-exam.app:30001 command on the controlplane node of cluster1.

student-node ~ ➜  k logs nginx-cka01-trb logs-container 
cat: can't open '/var/log/httpd/access.log': No such file or directory
cat: can't open '/var/log/httpd/error.log': No such file or directory

student-node ~ ➜  k logs nginx-cka01-trb 
logs-container   nginx-container  

student-node ~ ➜  k logs nginx-cka01-trb nginx-container 
Error from server (BadRequest): container "nginx-container" in pod "nginx-cka01-trb" is waiting to start: trying and failing to pull image

k get pods nginx-cka01-trb -o yaml > nginx-cka01-trb.yaml
vi nginx-cka01-trb.yaml :

spec:
  containers:
  - image: nginx:latest #(fix from "ltest" to "latest")

- command:
    - sh
    - -c
    - while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log || exit #(fix "httpd" to "nginx")
      1; sleep 30; done


k get all
NAME                       READY   STATUS    RESTARTS   AGE
pod/color-app-cka13-arch   1/1     Running   0          90m
pod/nginx-cka01-trb        2/2     Running   0          83s

k logs nginx-cka01-trb nginx-container 
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up

k logs nginx-cka01-trb logs-container 
2024/04/19 20:43:09 [notice] 1#1: using the "epoll" event method
2024/04/19 20:43:09 [notice] 1#1: nginx/1.23.3
2024/04/19 20:43:09 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2024/04/19 20:43:09 [notice] 1#1: OS: Linux 5.4.0-1106-gcp
2024/04/19 20:43:09 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2024/04/19 20:43:09 [notice] 1#1: start worker processes
2024/04/19 20:43:09 [notice] 1#1: start worker process 78
2024/04/19 20:43:09 [notice] 1#1: start worker process 79
2024/04/19 20:43:09 [notice] 1#1: start worker process 80
2024/04/19 20:43:09 [notice] 1#1: start worker process 81
........

k get services nginx-service-cka01-trb -o yaml > nginx-service-cka01-trb.yaml
vi nginx-service-cka01-trb.yaml:
apiVersion: v1
kind: Service
metadata:
  name: nginx-service-cka01-trb
  namespace: default
spec:
  clusterIP: 10.99.204.198
  clusterIPs:
  - 10.99.204.198
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - nodePort: 30001
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx-app-cka01-trb  # fix app name, get correct app name from deployment"
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}

k delete services nginx-service-cka01-trb 
service "nginx-service-cka01-trb" deleted

k apply -f nginx-service-cka01-trb.yaml 
service/nginx-service-cka01-trb created

curl http://kodekloud-exam.app:30001
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

========================